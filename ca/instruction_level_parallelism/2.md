在讲述之前首先要明白下方的两个公式：
- CPU性能 = 频率 * 单核IPC * 核数
- CPU功耗 = C * (V^2) * F
第1个公式描述了CPU性能与频率、IPC(Instruction Per Second)、核数之间的关系，第2个公式描述了CPU的功耗与频率、电压之间的关系。

发展史可以总结为这几个过程：
1. 在Free Lunch时代，提升性能主要依靠提高CPU频率和指令级并行。
2. 关于这段历史有两种解释，最后的结果都是CPU撞上了“功率墙”
    - 频率的不断提高门延迟也就不可忽略，所以还需要提高电压来减少门延迟，这就导致了功耗在后期随着频率的三次方而增加，而且指令级并行也因为功耗原因不能继续扩展。
    - 制程工艺不断下降，晶体管阈值电压可以降低，但是为了性能考虑就没有降低，所以后来功耗突飞猛进；再往后为了控制功耗就开始控制阈值电压了，但是阈值电压后来降不下去了，制程工艺仍在下降，所以功率密度还是大幅度上升，考虑封装与散热的话`150W/cm^2`是上限；后来为了在“功耗墙”限制下把频率拉高就牺牲了门翻转的比例。
3. 单核性能还是有上限，于是就想出将多个CPU连接来提供服务，这样功耗线性增长2倍，IPC也近似提升2倍，性能得到线性提升。在多核时代CPU的多核高效互联又成为了新的问题，因此诞生了星型连接、ring buffer连接、mesh连接等多种连接方式。
4. 将越来越多个CPU捆绑在一起时通信成本开销一直在上涨，所以后来又开发出了NUMA架构系统，这种系统通过QPI总线相互通信，可以大大增强CPU的线性扩展能力。

# Free Lunch时代
[![olo874.png](https://z3.ax1x.com/2021/11/30/olo874.png)](https://imgtu.com/i/olo874)

这个时代的逻辑是这样的：制程工艺水平提升->晶体管越做越小->晶体管之间通信距离持续缩短->频率不断提升，在这个时代里程序员不需要做很多努力，只要保持不断更新系统，那么他写的程序几乎不需要改进就能在新机器上跑的更快，所以又叫Free Lunch时代。

有两个定律引领着这个时代发展，分别是摩尔定律与登那德缩放定律：
- 摩尔定律：当价格不变时，集成电路（即芯片）上可容纳的元器件的数目，每隔约 18-24 个月便会增加一倍，性能也将提升一倍。
- 登那德缩放定律：在增加芯片组件的同时，缩小芯片体积将使芯片运行速度更快，同时降低生产成本和能耗。

为了保证门电路的延迟符合要求，所以电压有一个下限，当下限电压确定、制程工艺还在减少时功率密度会大幅增加，即使电压从5V讲到1.3V的过程中功率密度还是增加的，因为面积减小的速率比电压减少的要快（0.9>0.9*0.9）

根据封装工艺与散热要求，芯片的功率密度限制在`150W/cm^2`，这就导致了在制程工艺确定的情况下CPU的频率上限会是一个较低的数字，但是又想让CPU在不超过功率密度的情况下提高频率，那就只能控制门电路翻转的比例，一般是5%左右。

暗硅：功率预算不足，所以不能使所有的核心都能达到最高频率；在Intel CPU中会出现这种情况：单核可以睿频到3.7G，但是多核只能同时到2.7G就是这个原因

超频：超频可以通过加电压的方式来进行就是通过减少门电路延迟来实现的

# 榨干性能
这一时代首先是引入了CPU Cache来弥补“内存墙”，后来又使用Pipeline技术来增大指令吞吐，最后为了充分利用Pipeline又发明了分支预测技术与乱序执行技术。
## CPU Cache
参见《CPU Cache》

## 流水线
参见《CPU多级流水线》

## 分支预测
参见《CPU分支预测》

## 乱序执行
参见《CPU乱序执行》

# 多核时代
步入多核时代的时候也不是一帆风顺的，首先将多个核进行高效连接是一个问题，还有就是核越来越多通信成本越来越大时，就应该更换另一种连接方式。
## 核间通信
### 星型连线
在只有1个核心的情况下，其他周边节点可以星型的方式，围绕核心做星型连接。双核大概可以采用双星结构。但随着核心数增加，这种连接关系就会显得越来越复杂。在处理器一个die内的核心数达到4个的时候，核心之间的连接就会产生分歧了。以全连接的方式连接，则核心之间需要两两相连——听起来似乎也还好。而当核心数增加到6个的时候，全连接的复杂度显著增加。这种全连接方案当然能够达成最高的互联性能，包括带宽和延迟。但全连接也意味着设计复杂度、成本和功耗的增加。

[![o1iuCt.png](https://z3.ax1x.com/2021/11/30/o1iuCt.png)](https://imgtu.com/i/o1iuCt)

### ring buffer
Intel的环形总线Ring Bus通常是双环，数据流向是两个方向。环形总线和全连接方案相比，每两个模块之间的平均通讯距离实际上是更长的，最长的时候可能需要经过半个环。这就产生了延迟、带宽方面的变数。这种Ring Bus在实施复杂度、成本和功耗方面都达到了相对的平衡——尤其在核心数更多的情况下。如果4个模块做全连接，那么每个模块都要做3个连接，每两个模块之间的通讯长度是1跳。这4个模块若为双向环形通讯，则每个模块做2个连接，平均每两个模块之间的通讯长度是1.3跳。达到6个模块的时候，全连接每个模块就要做4个连接；环形连接时，每个模块依然是2个连接，平均通讯距离为1.8跳。

如前文所述，在考虑当代PC CPU多核心（比如现在高端桌面处理器是8个核心），以及DRAM控制器、I/O、核显之类的构成时，全连接的复杂性将变得难以为继。环形总线至此都还是权衡利弊的方案。但环形总线也不是万能的，当核心数进一步增加时，问题就会变得比较大了。在核心数增加到10个，甚至12个以后，ring也将变得很大，核心间的延迟将进一步增大；要喂给核心的数据带宽需求变大。这其实也是Intel当代的酷睿处理器很难在核心上可与AMD Ryzen去比的重要原因。所以10代酷睿处理器最多塞了10个核心，而11代则只塞了8个核心（与工艺限制有很大关系）。单die之下，再塞核心一方面会让die size变得过大，影响良率和成本；另一方面核间通讯效率也会大幅下降。

[![o1iK8P.png](https://z3.ax1x.com/2021/11/30/o1iK8P.png)](https://imgtu.com/i/o1iK8P)

### mesh
参考Arm面向服务器的Neoverse处理器IP：比如Neoverse N1，就核心微架构层面，它与手机上很多人熟知的Cortex A76是比较类似的，只是因为服务器处理器核心数可能会非常多（Arm这两代的最高配都预设了128个核心），自然不可能用环形或全连接方案。在2D mesh网络连接下，大致连接方案如上图所示，就像围棋棋盘一样互联。

[![o1iMgf.png](https://z3.ax1x.com/2021/11/30/o1iMgf.png)](https://imgtu.com/i/o1iMgf)

## NUMA
由于NUMA节点之间可以通过互联模块(如称为Crossbar Switch)进行连接和信息交互，因此每个CPU可以访问整个系统的内存(这是NUMA系统与MPP系统的重要差别)。显然，访问本地内存的速度将远远高于访问远地内存(系统内其它节点的内存)的速度，这也是非一致存储访问NUMA的由来。由于这个特点，为了更好地发挥系统性能，开发应用程序时需要尽量减少不同CPU模块之间的信息交互。利用NUMA技术，可以较好地解决原来SMP系统的扩展问题，在一个物理服务器内可以支持上百个CPU

[![o1im4I.png](https://z3.ax1x.com/2021/11/30/o1im4I.png)](https://imgtu.com/i/o1im4I)

# 新的问题
在制程工艺不断缩小的时候，新的问题又出现了，那就是量子隧穿效应，量子隧穿主要描述下图这种场景：电子的状态用电子云来描述，电子是有一定的概率穿过势垒，从而造成比特位翻转的。

[![o1pzNt.jpg](https://z3.ax1x.com/2021/11/30/o1pzNt.jpg)](https://imgtu.com/i/o1pzNt)
