# 为什么要有缓存？
## 访问时延
下图是各种CPU操作所需要的时钟周期数，最下面一行列出的是光经过上述时钟周期可以走的距离。

[![CPU访问各种存储介质的时延](https://z3.ax1x.com/2021/11/15/I2J2an.jpg)](https://imgtu.com/i/I2J2an)

## 内存墙
在过去的20多年中,处理器的性能以每年大约55%速度快速提升，而内存性能的提升速度则只有每年10%左右。长期累积下来，不均衡的发展速度造成了当前内存的存取速度严重滞后于处理器的计算速度，后来将这种严重阻碍处理器性能发挥的内存瓶颈命名为"内存墙"(Memory Wall)

在2015年Intel发布的一个报告中指出，“摩尔定律”失效的原因主要有3个方面:
1. 首先，随着芯片几何尺寸缩小及时钟频率提高，晶体管漏电流会增加，导致超额的功耗和热量；
2. 其次，因为存储器的访问时间无法跟上时钟频率的成长，导致更高的时脉速度所带来的优势被存储器的延迟抵消。
3. 再次，对于某些应用，随着处理器速度的提高，传统的序列架构变得越来越低效，进一步削弱了频率提高可带来的收益。
4. 此外，部分原因是由于固态组件内产生电感的方法受到限制，信号传输中的电阻-电容延迟会随着特征尺寸的缩小而增加，这就额外带来了频率增加也无法解决的瓶颈。


## 缓存
为了解决“内存墙”的问题，计算机科学家就想到了很简单的处理手段：加Cache。内存又叫做DRAM（Dynamic Random Access Memory），之所以叫Dynamic是因为内存需要“刷新”才能保证存储值，而有一种不需要刷新就能保存数据的存储器（还是需要通电，断电同样丢数据）叫做SRAM（Static Random Access Memory），这种存储器由于不需要刷新、速度更快所以就被选择作为DRAM与CPU之间Cache的存储介质。

在现代CPU中一般有3级缓存，分别是：L1缓存（包含L1i缓存，L1d缓存），L2缓存，L3缓存；L1缓存是指令缓存，L1d是数据缓存；L1、L2缓存一般是每个物理核独占，L3是所有核共享，L3在使用场景中最大的用途是减少数据回写内存的频率，加速多核心之间的数据同步。

1. DRAM的刷新是怎么回事？<br/>
    DRAM物理存储单元是电容器，电容器充满电后代表 1（二进制），未充电的代表 0。由于电容器或多或少有漏电的情形，若不作特别处理，电荷会渐渐随时间流失而使资料发生错误。刷新是指重新为电容器充电，弥补流失了的电荷。DRAM的读取即有刷新的功效，但一般的定时刷新并不需要作完整的读取，只需作该芯片的一个列（Row）选择，整列的资料即可获得刷新，而同一时间内，所有相关记忆芯片均可同时作同一列选择，因此，在一段期间内逐一做完所有列的刷新，即可完成所有存储器的刷新。
2. 既然SRAM这么好，为什么内存不都使用SRAM？<br/>
    因为SRAM成本比较贵，而且存储1bit就需要4个场效应管，占据的空间比较大，所以就不会拿来做主存。
3. 为什么要对L1缓存进行数据、指令分离？<br/>
    指令的特点与数据不太相同，指令刷新率更低而且不会被覆盖。其实从L1的角度向上看CPU就不是冯诺依曼架构了，而是哈佛架构，这两种架构的区别就是数据与指令是否统一存放。
4. L3的inclusive实现与non-inclusive实现是怎么回事？<br/>
    以Intel的Skylake架构为例，Skylake的缓存设计使用了Non-inclusive的架构设计，同时调大了L2缓存的大小。简单来说，区别就是以前从内存读入的数据会同时进入L3和L2缓存，而现在会从内存直接进入L2缓存，当该数据从L2缓存中清除的时候，才会进入L3缓存。在以前的缓存架构中，L2缓存中的数据是L3缓存数据的真子集。而Skylake的架构中，L2和L3的数据在绝大部分时间中是独占的(exclusive)的，即二者所缓存的数据没有重叠。但注意，Skylake的架构是Non-inclusive，而不是严格的exclusive，是因为当同一条Cache Line在多个CPU核的L2缓存中都存在时，L3缓存中也有该Cache Line的数据。


# 内存->缓存的映射关系？
## 直接映射
比如一共有8个cacheline位置，那么编号为7的位置就只会放（内存地址）% 8 = 7 的内存数据

加载一个cacheline的流程：
1. 拿到内存地址，计算（地址 % 可以容纳的cacheline数量）= 所在位置
2. 检查invalid位
3. 取数据


优点：
1. 简单

缺点：
1. 竞争比较激烈，会有比较频繁的缓存换入换出
2. 缓存占用率和命中率都不高

![image](https://user-images.githubusercontent.com/56379080/147403010-a722e00c-8fdc-4a92-af0e-01d4ed3f575a.png)

## 全相连映射
内存里的任何一个cacheline都可以放在缓存的任何一个位置

加载一个cacheline的流程：
1. 拿到内存地址
2. 遍历cache中所有的cacheline的TAG，直到找到匹配的
3. 检查invalid位
4. 取数据


优点：
1. 利用率高

缺点：
1. 实现较复杂

![image](https://user-images.githubusercontent.com/56379080/147403018-2333ac0e-be74-4b59-9e14-ed050a79635d.png)

## 组相连映射
放到固定的组中，但是组内位置不确定

加载一个cacheline的流程：
1. 拿到内存地址，计算所在的组
2. 组内进行遍历查找
3. 检查invalid位
4. 取数据


![image](https://user-images.githubusercontent.com/56379080/147403144-688e0426-93af-403d-b940-1f04e297055b.png)


# 怎样保证缓存一致性？
## MESI
在介绍MESI协议之前还需要知道一个概念：CacheLine，CacheLine是内存和CPU Cache之间交互的最小单位，内存和CPU Cache的访问统一都是64Byte对齐的。也就是说即便你只需要读取1bit的数据，CPU还是会把64Byte的数据从内存都扔到L1

由于L2 Cache是每个物理核独占的，所以每个核之间有可能会有数据重叠，对于只读操作来说这完全没有问题，但是CPU不可能只是读数据，还需要写数据，这时候就会造成数据出错，这时候就需要一个缓存一致性协议来保证数据没问题，一般实现在CPU内部的就是MESI协议或者它的变体。

MESI协议使用状态机控制了各种缓存之间的状态变化，从而实现缓存一致性。
1. M 修改 (Modified)
该Cache line有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。监听任务：缓存行必须时刻监听所有试图读该缓存行相对就主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S（共享）状态之前被延迟执行。
2. E 独享、互斥 (Exclusive)
该Cache line有效，数据和内存中的数据一致，数据只存在于本Cache中。监听任务：缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S（共享）状态。
3. S 共享 (Shared)
该Cache line有效，数据和内存中的数据一致，数据存在于很多Cache中。监听任务：缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。
4. I 无效 (Invalid)
该Cache line无效。

## invalidate-queue与store-buffer
如果完全按照MESI协议来执行的话是不会出现可见性问题的，但是坏就坏在MESI协议执行的太慢了，所以CPU设计者就又加了缓存与缓冲。CPU就变成了下面这个样子：

[![I2svWQ.png](https://z3.ax1x.com/2021/11/15/I2svWQ.png)](https://imgtu.com/i/I2svWQ)

当一个CPU进行写入时，首先会给其它CPU发送Invalid消息，然后把当前写入的数据写入到Store Buffer中。然后异步在某个时刻真正的写入到Cache中。当前CPU核如果要读Cache中的数据，需要先扫描Store Buffer之后再读取Cache。但是此时其它CPU核是看不到当前核的Store Buffer中的数据的，要等到Store Buffer中的数据被刷到了Cache之后才会触发失效操作。而当一个CPU核收到Invalid消息时，会把消息写入自身的Invalidate Queue中，随后异步将其设为Invalid状态。和Store Buffer不同的是，当前CPU核心使用Cache时并不扫描Invalidate Queue部分，所以可能会有极短时间的脏读问题。MESI协议，可以保证缓存的一致性，但是无法保证实时性。
