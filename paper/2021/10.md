这是NSDI 2021的best paper，来自CMU和Twitter的合作，一作的杨骏骋大佬本科和硕士学的都是化学，然后在15年化学硕士毕业之后读了计算机的硕士，17年开始在CMU读计算机的博士，他的另一篇文章《Kangaroo: Caching Billions of Tiny Objects on Flash》也中了SOSP 2021的best pape，只不过不是一作。

论文主要的思想就是通过牺牲TTL的精确性（缩小TTL范围）来做batch，以达到高效、省空间的目的；文中作者还分析KV Store和KV Cache的区别，主要体现在3个方面：cache系统会有TTL，store系统一般不会有TTL；cache系统可以根据策略自主清除一些数据以节省空间，store系统不允许这样做；cache系统的设计偏爱速度和高带宽，store系统设计偏向于保证一致性情况下的时延和带宽

# 基本概念
## TTL
TTL：Time To Live
TTL值一般在写入缓存系统的时候指定，控制着缓存的有效时间，超出TTL范围的cache一定不能返回，一般有这几种用法：
1. 使用TTL限制数据一致性，过期就需要从数据库再拿
2. 在推荐系统中，一般使用基于窗口的计算方法（例如只计算最近7天的爱好，超过7天的历史数据就要被清除）
3. 用来限流 ，防止DOS攻击等

## LRU与LFU
- LRU：根据访问时间来决定置换顺序
- LFU：根据访问频率来决定置换顺序

## Expiration And Eviction
expiration：删除那些在未来不再会被访问到的缓存，言下之意就是过期了
eviction：删除那些还在有效期内的缓存，可能因为缓存满了等各种原因

# Background And Motivation
目前基于内存的Cache或多或少都有这3方面的问题：
1. 并不能很及时的清除过期的cache
2. 每个cache object都有metadata，开销很大
3. 内存碎片很严重

## Expiration Policy
1. Lazy：在Get的时候检查一下是否在TTL内，不在的话就删除且不返回
2. Proactive：
  - Checking LRU：从LRU链表末端删除数据，线程扩展性不好
  - Transient Object Pool：短期对象池
  - Full Cache：全量扫描，如果一些cache的TTL比较长的话就会浪费资源（扫了很多遍，依旧存在）
  - Random Sampling：随机采样，效果比全量扫描还要差，因为全是Random Access，对Cache极度不友好

## Metadata
推特内部cache的典型大小是230B、55B、294B和72B，但是Memcached为每个cache都存储了56B的元信息，这将是巨大的浪费。
RAMCloud使用LSM结构来减少metadata的存储开销

## Memory Fragmentation
SLAB：Memcached采用的内存分配方式，会引入SLAB“固化”的问题：一个Class以先到先得的方式占领了一个Slab分配器，然后不再申请或者申请很少，就会造成浪费

[![oQgK2Q.png](https://z3.ax1x.com/2021/11/29/oQgK2Q.png)](https://imgtu.com/i/oQgK2Q)

## Throughput And Scalability
本质上是锁的开销非常大

# Design
设计遵守的3条原则：
1. Be proactive， don't be lazy.
2. Maximize metadata sharing
3. Perform Macro management：以批的方式管理

## 总体架构
[![oQglKs.png](https://z3.ax1x.com/2021/11/29/oQglKs.png)](https://imgtu.com/i/oQglKs)

## TTL bucket
[![oQgu8g.png](https://z3.ax1x.com/2021/11/29/oQgu8g.png)](https://imgtu.com/i/oQgu8g)

- 一共1024个bucket， bucket负责的TTL区间不是均匀的，第一个bucket时间跨度为8，后面会依次增大，这是建立在“一般cache设置TTL在1天内”这一观察之上
- 在一个bucket内部，会以TTL区间左端的时间为准，举例来说，在第2个bucket里面所有的cache都会在第9秒过期，即使他的TTL是16或者其他的，这是建立在cache可以eviction的基础之上的（可以提前失效，不能过期失效）
- TTL bucket很小，几乎可以都放进缓存中
- 保存了segment链表的头指针与尾指针

## Object Store
[![oQgMvj.png](https://z3.ax1x.com/2021/11/29/oQgMvj.png)](https://imgtu.com/i/oQgMvj)

- 每个segment大小为1MB
- 每个对象先计算属于哪个TTL bucket，然后以追加写的方式放到segment里面
  - segment满了就申请新的segment
  - 内存满了就启动eviction
- segment链表上的对象都是按照先来后到排序的

## Hash Table
[![oQgnPS.png](https://z3.ax1x.com/2021/11/29/oQgnPS.png)](https://imgtu.com/i/oQgnPS)

- 简单chained对象的方式限制了扩展性（random access）而且存放指针过多，所以采用bulk chained方式
- 第1个slot存放bucket信息
	- 1B的spin lock
	- 1B的slot counter
	- 2B的last access timestamp：每秒最多更新1次来应对短时间大量访问
	- 4B的CAS value
- 其余slot存放cache位置信息
	- 3B的segment id
	- 20bit的segment内部offset
	- 1B的频率计数器
		- 前4bit存放精确计数
		- 当计数大于16时使用Morris计数器：以1/2^X的概率加1
	- 12bit的tag：是K的简单Hash，用于比较

## Metadata
- 同一个segment共享：创建时间、TTL、引用计数
	- 引用计数估计是用来判断是否有人在使用此segment，没有的话就可以回收了
- 同一个hash table bucket共享：last-access timestamp、spinlock、cas value、hash pointer
	- cas value是为了支持bucket里面的CAS操作（原子删除）
	- cas value在hash bucket级别共享不会对性能有很大的影响：CAS操作比较少、hash bucket里面也不会有很多的元素、大不了重试
- 在K为1B、V为3B、FLAG为1B的情况下，每个KV对仅占用5B（不是平均，就是单个）

## 读写流程
写KV：
1. 按照TTL bucket找到segment
2. 追加写到segment里面
3. 写入到hash table里面

读KV：
1. 从hash table里面找到segment
2. 去segment里面读

## Expiration
1. 后台线程逐个对TTL bucket进行扫描
2. 再对segment list进行扫描，如果过期就释放整个segment

## Eviction
使用MergeBased Eviction算法，如下图，合并t2 t3 t4三个segment
[![oQg1rn.png](https://z3.ax1x.com/2021/11/29/oQg1rn.png)](https://imgtu.com/i/oQg1rn)


为什么每个segment在新的segment中贡献相同？没有证据证明谁比谁更有效
为什么选择临近的segment呢？从下图可以得出结论：纵坐标为相关性
[![oQg3bq.png](https://z3.ax1x.com/2021/11/29/oQg3bq.png)](https://imgtu.com/i/oQg3bq)

- 只从相同TTL bucket里面拿segment进行合并，合并完成放在原地
- 选择KV的依据是频率与大小的比值
	- 频率需要再去Hash table中拿
- 有一个动态的threshold来控制哪些KV可以存活下去
	- 在每个segment里面，每遍历（1/10）* KV_NUM 次就更新一次
	- 最终目的是每个segment选出1/N个KV进入到新的segment中
	- 猜测运行方式：初始化一个threshold，大于threshold的就加入到新的segment中，小于的就不加入，每运行（1/10）* KV_NUM次就计算一下（期望数目 - 实际数目），如果大于0就调小threshold，小于 0就调大threshold，再进行遍历下一个（1/10）KV对
- 合并完之后会把频率清零防止污染

# Experiment
## 数据集
[![oQgGV0.png](https://z3.ax1x.com/2021/11/29/oQgGV0.png)](https://imgtu.com/i/oQgGV0)

## Miss
[![oQgJaV.png](https://z3.ax1x.com/2021/11/29/oQgJaV.png)](https://imgtu.com/i/oQgJaV)

large cache：当cache size增大时，miss ratio增长缓慢，这时候的cache size叫做large cache大小
small cache：large cache大小的一半

## metadata
miss ratio随着metadata大小的变大而变大
[![oQgY5T.png](https://z3.ax1x.com/2021/11/29/oQgY5T.png)](https://imgtu.com/i/oQgY5T)

## 内存占用
想研究在同样miss ratio的情况下谁占用的内存空间比较小
[![oQgNPU.png](https://z3.ax1x.com/2021/11/29/oQgNPU.png)](https://imgtu.com/i/oQgNPU)

## 吞吐
[![oQgUGF.png](https://z3.ax1x.com/2021/11/29/oQgUGF.png)](https://imgtu.com/i/oQgUGF)

## 扩展性
[![oQga24.png](https://z3.ax1x.com/2021/11/29/oQga24.png)](https://imgtu.com/i/oQga24)

## 敏感度分析
[![oQga24.png](https://z3.ax1x.com/2021/11/29/oQga24.png)](https://imgtu.com/i/oQga24)
- 多少个segment合并最好？
	- 3-4个最好
- 合并时更少的segment为什么不行？
	- 合并2个可能会使新生成的segment空间浪费（填不满）
	- 小尺度上去预测（谁将来会被使用到）不太准确
- 更多的segment为什么不行？
	- 提高了进入新segment的门槛
- miss ratio与段大小没有关系，因为从每个段中保留的对象的比例不依赖于段的大小
