[![oiaPRx.png](https://z3.ax1x.com/2021/11/24/oiaPRx.png)](https://imgtu.com/i/oiaPRx)

[![oiaELD.png](https://z3.ax1x.com/2021/11/24/oiaELD.png)](https://imgtu.com/i/oiaELD)
今天我要讲的论文题目是Rethinking File Mapping in Persistent Memory，这篇论文的背景就是持久化内存已经正式商用，它有着比SSD快30到40倍的速度，但受限于文件系统的IO性能PM的性能并没能全部发挥。

[![oiaAsO.png](https://z3.ax1x.com/2021/11/24/oiaAsO.png)](https://imgtu.com/i/oiaAsO)
因此本文就针对文件系统做了一些优化与测试。首先要介绍的是传统的文件IO方式，比如现在要向notes.txt的0x4000000处写一个字符串，首先要拿到文件的inode编号与逻辑偏移，然后File mapping模块与block allocator模块进行交互来申请page，之后通过DMA方式将File mapping需要的数据结构读入到page cache中，最后从page cache中读取数据结构并得到实际的物理地址。由于底层磁盘速度很慢，所以File mapping操作的消耗并不会很明显。File mapping操作可以理解为将文件的逻辑地址转化为实际的设备地址的操作。

[![oiakQK.png](https://z3.ax1x.com/2021/11/24/oiakQK.png)](https://imgtu.com/i/oiakQK)
典型的在慢速块设备上使用的File Mapping数据结构是extent tree和redix tree，这两种属于per-file mapping，意思就是每个文件都会有一个属于自己的数据结构。第一种数据结构是extent tree，这种数据结构有两种类型的节点，分别为indirect node和direct node，当我们要查找21号逻辑块的时候首先需要在indirect node中进行二分查找，查找到的就是数据块所在的direct node，然后再在direct node中进行二分查找，查找到真实的物理块地址。direct node的格式为：<逻辑块起始地址：物理块起始地址（块的数目）>，这后面之所以还要有一个<块的数目>这个参数是为了减少在大量连续情况下的direct node中节点的数目。还有一种数据结构是radix tree，这种数据结构使用的是按位操作来定位，比如说我们现在要找到第21号逻辑块的物理块号，首先要将21变为二进制，由于每个块大小是4KB，每个块内节点大小是8B，所以每一层使用逻辑块号9位信息，这样的话根据每9位定位一个level就可以直接定位到最顶层，最底层保存的是实际的物理地址，在这个图中可以得到第21个逻辑块的实际物理块号是100。两种传统数据结构相比的话各有优劣，radix在每一层需要更少的内存读取次数但是却需要多层跳转。

[![oiaiz6.png](https://z3.ax1x.com/2021/11/24/oiaiz6.png)](https://imgtu.com/i/oiaiz6)
作者认为现在设备变得很快，之前的很多的工作也把很多组件变得很快，但是对于file mapping都没有足够的重视，这也是作者题目为rethinking file mapping的原因，所以作者重新针对PM设计了File mapping数据结构。

[![oiaZee.png](https://z3.ax1x.com/2021/11/24/oiaZee.png)](https://imgtu.com/i/oiaZee)
这就是作者涉及的两种global hash File mapping数据结构，之所以是global hash File mapping数据结构是相对于之前的每个文件都有自己的数据结构而言的，在这里全局只有一个数据结构。第一个数据结构是Cuckoo Hashing file mapping，因为是全局数据结构所以在进行file mapping的时候不仅仅需要传入逻辑块号还需要传入文件唯一描述符inode num。这个数据结构的查找与插入操作与cuckoo hash table一样，唯一不同的是后缀有一个参数用来显示物理块上的连续状态，这里作者解释使用cuckoo hash算法的原因是考虑到cuckoo hash有读取次数上限的好处。第二个数据结构就是作者设计并优化的Hash FS，这个数据结构使用线性探查法来构造hash table，之所以使用线性探查法是因为考虑到要利用PM buffer的局部性原理，这里没有物理块号的显示是因为可以直接由索引块的物理位置求出数据的物理块的位置，HashFS的优越性体现在使用线性探查法来利用局部性原理，还消除了cuckoo hash的重分配操作。

[![oiaedH.png](https://z3.ax1x.com/2021/11/24/oiaedH.png)](https://imgtu.com/i/oiaedH)
作者也做了一系列实验来证明它实现的系统的优越性。设备是128GB的AEP，系统在Strta之上进行设计与实现。这里作者还引入了一个参数layout-score来控制文件的碎片化程度，当layout-score是1的时候意味着所有的块都是连续的，当layout-score是0的时候意味着所有的块都不是连续的，如果没有说明的话默认layout-score是0.85。

[![oiamod.png](https://z3.ax1x.com/2021/11/24/oiamod.png)](https://imgtu.com/i/oiamod)
[![oiauFA.png](https://z3.ax1x.com/2021/11/24/oiauFA.png)](https://imgtu.com/i/oiauFA)
- 为什么per-file mapping hot cache写都有很低的时延？
  因为SIMD预取与指针优化

- 为什么在cold cache的情况下差距这么大？
  因为树的操作是很耗时的，需要很多次的内存读取，基于hash的最多需要2次内存读取

- 在cold cache情况下为什么插入这么耗时？
  主要消耗在块分配上，同时具有易失和持久的数据结构，所以在最后一级导致cache miss
[![oiaKJI.png](https://z3.ax1x.com/2021/11/24/oiaKJI.png)](https://imgtu.com/i/oiaKJI)
作者测试了上述四个数据结构的在cold cache和hot cache的情况下的读写性能表现，作者没有使用时间来计时，而是使用时钟周期来计时，最后结果表明基于Hash的file mapping方式性能要好于传统的，尤其是在随机访问场景下。作者还测试了cold cache场景下碎片化对系统的影响，实验发现基HashFS不会受到碎片影响。

- 为什么extent tree变化这么大？
  因为如果不呈现碎片化的话，extent tree很小，很方便遍历

- 为什么同样是基于hash的操作，cuckoo hash的操作也会体现出变化？
  可以看到查询并没有变化很大，在插入阶段差距很大应该是发生了hash冲突，然后cuckoo hash要进行迁移，所以耗时比较多。


作者还测试了在文件大小在4KB、4MB、4GB情况下的读写时延，从中我们可以得出结论per-file structure会随着文件的增大而增大从而导致更多的跳转操作，从而增加时延。基于hash的数据结构并没有因为文件大小增大而性能下降。基于cuckoo hash的结构具有高时延是情理之中。还测试了在IO大小在4KB、64KB、1MB情况下的时延。
[![oiaMWt.png](https://z3.ax1x.com/2021/11/24/oiaMWt.png)](https://imgtu.com/i/oiaMWt)
- 为什么这个实验要以比率来作为标准而不像之前那样直接使用时延？
  随着IO size的增大时延一定会增高，为了剔除io size对时延的影响，选择了比率

- 为什么时延反而会下降？
  Radix与extent会在叶子结点一次定位很多block，所以在大io的情况下时延反而有提升。
[![oialSP.png](https://z3.ax1x.com/2021/11/24/oialSP.png)](https://imgtu.com/i/oialSP)
[![oia1Qf.png](https://z3.ax1x.com/2021/11/24/oia1Qf.png)](https://imgtu.com/i/oia1Qf)
[![oia3y8.png](https://z3.ax1x.com/2021/11/24/oia3y8.png)](https://imgtu.com/i/oia3y8)
[![oia8OS.png](https://z3.ax1x.com/2021/11/24/oia8OS.png)](https://imgtu.com/i/oia8OS)
[![oiaJeg.png](https://z3.ax1x.com/2021/11/24/oiaJeg.png)](https://imgtu.com/i/oiaJeg)

使用Strata系统的默认mapping方式与优化（no page-cache、在PM上直接操作）的extent tree进行比较，很清晰的看到去掉了Page cahce性能有了明显的提升，这是因为少了DRAM的开销，还缩短了IO路径长度。
[![oiaYwQ.png](https://z3.ax1x.com/2021/11/24/oiaYwQ.png)](https://imgtu.com/i/oiaYwQ)

作者还使用了YCSB负载来测试Hash FS的性能，可以从图中看到A、E测试中几个数据结构的差距并不是很大，猜测的原因是因为LevelDB自身的瓶颈，而且可以测出在Strata系统中花费在File Mapping上的开销占了总共的70%。第一个实验是在4线程的情况下测试的，作者还进行了一个单线程的实验，发现多线程情况下差距会更大（HashFS 1线程与4线程间性能差距45%，Per-file1线程与4线程差距23%），具体原因是因为Strata的page cache有一把全局锁
[![oiaUFs.png](https://z3.ax1x.com/2021/11/24/oiaUFs.png)](https://imgtu.com/i/oiaUFs)
[![oiatoj.png](https://z3.ax1x.com/2021/11/24/oiatoj.png)](https://imgtu.com/i/oiatoj)
