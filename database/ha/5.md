# 配置变更
在Raft里面要想实现配置变更主要有以下三种方式：
1. 关闭集群，更改配置，然后再启动集群
2. 每次都单节点的增删节点
3. 使用Joint Consensus来实现增删多节点

# （增/删）单节点
## 步骤
1. 向Leader提交一个成员变更请求，请求的内容为服务节点的是添加还是移除，以及服务节点的地址信息
2. Leader在收到请求以后，回向日志中追加一条ConfChange的日志，其中包含了Cnew，后续这些日志会随着AppendEntries的RPC同步所有的Follower节点中
3. 当ConfChange的日志被添加到日志中是立即生效（注意：不是等到提交以后才生效）
4. 当ConfChange的日志被复制到Cnew的Majority服务器上时，那么就可以对日志进行提交了

## 证明
下图就是Raft论文中的配图，能够实现单节点配置变更的关键在于：**C<sub>old</sub>的Majority和C<sub>new</sub>的Majority之间一定存在交集**，而两个配置文件存在交集就意味着相交的那个元素要么归属Old配置要么归属New配置（一个节点在一个Term里面只能投一次票），所以集群中只会有一个多数派，也就是说只会有一个Leader，这样就可以保证Raft协议正确执行。

**即使集群中存在两份不同的配置文件，但是仍然能且仅能选举出一个Leader**

![image](https://user-images.githubusercontent.com/56379080/184103573-d5a36e03-55c6-4366-adfa-9032739fda05.png)

### 增加节点情况
下面给出在3节点集群上增加1个节点的情况：
```
Old: S1, S2, S3        Majority Need:2
New: S1, S2, S3, S4    Majority need:3
// 此时在Old中实现Majority可能需要S1，S2；在New中实现Majority可能需要S2，S3，S4，存在交集
// 此时Old配置文件中的Majority与New配置文件中的Majority一定存在交集
```

下面给出在4节点集群上增加1个节点的情况：
```
Old: S1, S2, S3, S4        Majority Need:3
New: S1, S2, S3, S4, S5    Majority need:3
// 此时在Old中实现Majority可能需要S1，S2, S3；在New中实现Majority可能需要S3，S4，S5，存在交集
// 此时Old配置文件中的Majority与New配置文件中的Majority一定存在交集
```

### 删除节点情况
下面给出在4节点集群上删除1个节点的情况：
```
Old: S1, S2, S3, S4    Majority need:3
New: S1, S2, S3        Majority Need:2
// 此时在Old中实现Majority可能需要S2，S3，S4；在New中实现Majority可能需要S1，S2，存在交集
// 此时Old配置文件中的Majority与New配置文件中的Majority一定存在交集
```

下面给出在5节点集群上删除1个节点的情况：
```
Old: S1, S2, S3, S4, S5    Majority need:3
New: S1, S2, S3, S4        Majority Need:3
// 此时在Old中实现Majority可能需要S3，S4，S5；在New中实现Majority可能需要S1，S2, S3，存在交集
// 此时Old配置文件中的Majority与New配置文件中的Majority一定存在交集
```

## 可能导致的问题
### 可用性问题
在我们向集群添加或者删除一个节点以后，可能会导致服务的不可用，比如向一个有三个节点的集群中添加一个干净的，没有任何日志的新节点，在添加节点以后，原集群中的一个Follower宕机了，那么此时集群中还有三个节点可用，满足Majority，但是因为其中新加入的节点是干净的，没有任何日志的节点，需要花时间追赶最新的日志，所以在新节点追赶日志期间，整个服务是不可用的。

下面使用具体的例子来说明一下：
```
时刻1：S1 S2  S3
时刻2：S1 S2  S3  S4
时刻3：S1 S2 [S3] S4      ---S3断电停机

```

在时刻1的时候集群中有3个节点，Majority是2；在时刻2的时候S4新加入集群，集群节点总数来到4，Majority是3；在时刻3的时候S3因为断电离开集群，集群中还有3台机器可以提供服务，满足Majority；但是因为S4是新加入的节点，所以S4会拒绝AppendEntries的RPC请求，这个时候集群对外无法提供服务，因为每个请求都需要3个节点（Majority）的应答，而S4由于节点上没有日志并不会应答；写请求肯定不会应答，因为日志都没匹配的上AppendEntries RPC都会直接报错，Leader就不会拿到多数派的响应，从而也不能对外提供服务；读请求按照道理来说也不会提供应答，因为按照前文说的线性一致性读方案来说，Leader在返回读的时候并不能直接返回本地结果，还需要与集群中多数进行一次心跳才能确认自己是Leader，在具体的实现中心跳就是使用AppendEntries RPC来实现的，因此读也不可以提供服务，但是如果把心跳RPC独立分开来应该就可以实现读

为了解决这个问题引入了Learner节点角色，Learner只会同步日志而不参与投票，所以也不会影响Majority数量的判断，等到Learner拿到所有的日志之后就可以切换角色加入集群

### Leader退位
Leader如果在收到最新的配置之后（最新配置不包含Leader所在节点）就直接shutdown会引发问题：日志并没有复制到大多数节点上，所以可以在配置提交（复制到多数节点上）之后再shutdown

### 移除Follower节点
1. 新配置被传输到Leader节点上，立即生效（某个节点被移除）
2. 被移除的节点上并没有收到最新的配置，它还以为自己还在集群中，但是Leader已经知道他被移除了，所以不会向他发送心跳
3. 这个被移除的节点会因为没有收到心跳而进行超时选举，当前Leader会退位
4. 这个被移除的节点并不能选举成功（把配置当做日志来看的话，因为不含有最新日志所以不会成功），Leader还是要在其它节点之间选出，这样就造成了很差的可用性

解决方法：
1. PreVote算法
2. 臣服算法

一种比较直观的方式是采用Pre-Vote方式，在任何节点发起一轮选举之前，就应该提前的发出一个Pre-Vote的RPC询问是否当前节点会同意给当前节点投票，如果超过半数的节点同意投票，那么才发生真正的投票流程的，有点类似于Two-Phase-Commit，这种方式在正常情况下，因为被移除的节点没有包含Cnew的ConfChange日志，所以在Pre-Vote情况下，大多数节点都会拒绝已经被移除节点的Pre-Vote请求。但是上面只能处理大多数正常的情况，如果Leader收到Cnew的请求后，尚未将Cnew的ConfChange日志复制到集群中的大多数，Cnew中被移除的节点就超时开始选举了，那么Pre-Vote此时是没有用的，被移除的节点仍有可能选举成功。顺便一说，这里的Pre-Vote虽然不能解决目前的问题，但是针对脑裂而产生的任期爆炸式增长和很有用的，这里就不展开讨论了。

如果每一个服务器如果在ElectionTimeout内收到现有Leader的心跳（换句话说，在租约期内，仍然臣服于其他的Leader），那么就不会更新自己的现有Term以及同意投票。这样每一个Follower就会变得很稳定，除非自己已经知道的Leader已经不发送心跳给自己了，否则会一直臣服于当前的leader，尽管收到其他更高的Term的服务器投票请求。




